# airflow-values.yaml
# Airflow 버전 2.8.1 이상을 권장합니다.
# https://github.com/apache/airflow/tree/main/chart/values.yaml 에서 최신 버전 확인

images:
  airflow:
    repository: hahxowns/airflow_add_pip
    tag: 3.0.6-amd2
    pullPolicy: IfNotPresent

# KubernetesExecutor 사용
executor KubernetesExecutor 


# airflow:
#   extraPipPackages:
#     - "apache-airflow-providers-apache-kafka==1.10.0"
    

fernetKeySecretName: "k8sa-airflow-fetnet-key"
#jwtSecretName: "k8sa-airflow-jwt-key"
webserverSecretKeySecretName: "k8sa-airflow-jwt-key"
# # Kafka 연결을 위한 환경변수 설정
# config:
#   AIRFLOW__CORE__EXECUTOR: KubernetesExecutor
#   AIRFLOW__KAFKA__BOOTSTRAP_SERVERS: "my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"
#   AIRFLOW__KAFKA__TEST_TOPIC: "test.airflow.kafka"

# # Kafka Connection 설정
# connections:
#   - conn_id: "kafka_default"
#     conn_type: "kafka"
#     host: "my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"


# DAG 동기화 설정 (Git Sync)
dags:
  # Git Sync 설정
  gitSync:
    enabled: true
    repo: https://github.com/KOSA-CloudArchitect/airflow.git
    branch: main
    subPath: dags
    # subPath를 제거하여 Git 저장소 루트의 dags 폴더를 /opt/airflow/dags에 마운트


# 외부 Postgres 사용 (차트 내 postgresql 서브차트)
data:
  metadataConnection:
    user: "postgres"
    pass: "NewSecurePassword2024!"
    protocol: postgresql
    host: "airflow-metadata-db.cxkoo6wi2dyy.ap-northeast-2.rds.amazonaws.com"
    port: 5432
    db: "airflow"
    sslmode: "require"
    # Add custom annotations to the metadata connection secret



# # 외부 데이터베이스 설정 (PostgreSQL)
postgresql:
  enabled: false # 차트 내장 DB 사용 안 함


logs:
  persistence:
    enabled: false
  
triggerer:
  persistence:
    enabled: false
  
  env:
    - name: PYTHONPATH
      value: "/opt/airflow/dags/repo/dags:/opt/airflow/dags"

    - name: AIRFLOW__LOGGING__LOGGING_LEVEL
      value: "INFO"
  
  serviceAccount:
    create: false
    name: airflow-irsa

# airflow-irsa = S3 IAM Role Account
workers:
  serviceAccount:
    create: false
    name: airflow-irsa
  
  env:
    - name: PYTHONPATH
      value: "/opt/airflow/dags/repo/dags:/opt/airflow/dags"


apiServer:
  serviceAccount:
    create: false
    name: airflow-irsa

dagProcessor:
  serviceAccount:
    create: false
    name: airflow-irsa
  
  env:
    - name: PYTHONPATH
      value: "/opt/airflow/dags/repo/dags:/opt/airflow/dags"

webserver:
  defaultUser:
    enabled: true
    username: admin
    password: admin
  
  env:
    - name: PYTHONPATH
      value: "/opt/airflow/dags/repo/dags:/opt/airflow/dags"


# scheduler:
#   command:
#     - "bash"
#     - "-c"
#     - "pip install apache-airflow-providers-apache-kafka && exec airflow scheduler"

# dagProcessor:
#   command:
#     - "bash"
#     - "-c"
#     - "pip install apache-airflow-providers-apache-kafka && exec airflow dag-processor"

# triggerer:
#   command:
#     - "bash"
#     - "-c"
#     - "pip install apache-airflow-providers-apache-kafka && exec airflow triggerer"



# logging 설정
config:
  logging:
    remote_logging: "True"
    remote_base_log_folder: "s3://hihypipe-airflow-logs/log"
    remote_log_conn_id: "s3_conn"

connections:
  - id: s3_conn
    type: s3
    extra: |
      {
      "region_name": "ap-northeast-2"
      }


# triggerer:
#   replicas: 1
#   resources:
#     requests:
#       cpu: "50m" # 리소스 요청을 낮춰봅니다.
#       memory: "128Mi"

# Airflow Webserver, Scheduler, Worker 리소스 설정 (필요에 따라 조절)
# 예시:
# scheduler:
#   resources:
#     requests:
#       cpu: "500m"
#       memory: "1Gi"
#   replicas: 2 # 고가용성 설정


